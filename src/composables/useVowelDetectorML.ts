import { ref, shallowRef, onUnmounted } from 'vue';
import * as tf from '@tensorflow/tfjs';
import type {
  Vowel,
  VowelDetectorConfig,
  VowelDetectionResult,
  DetectionStatus,
  VowelDetectedCallback,
  SilenceCallback,
  ErrorCallback,
  VowelDetectorHookReturn,
  VowelDetectorDebugData
} from '@/types/game';
import { DEFAULT_VOWEL_DETECTOR_CONFIG, DEFAULT_VOWEL_FORMANTS } from '@/config/vowels';

const VOWEL_CLASSES = ['A', 'E', 'I', 'O', 'U', 'silence'] as const;
const INPUT_SAMPLES = 3360; // 210ms @ 16kHz
const TARGET_SAMPLE_RATE = 16000; // è®­ç»ƒæ¨¡å‹ä½¿ç”¨çš„é‡‡æ ·ç‡

// Vite ä¼šå°† TS worker ç¼–è¯‘æ‰“åŒ…ï¼Œè¿”å›å¯ç”¨ URL
const workletUrl = new URL('../workers/vowelAudioProcessor.ts', import.meta.url);
const HYSTERESIS_HIGH = 0.6;
const HYSTERESIS_LOW = 0.45;
const SWITCH_MARGIN = 0.08;
const SUSTAINED_RE_EMIT_INTERVAL = 300; // æŒç»­å‘åŒä¸€å…ƒéŸ³æ—¶ï¼Œæ¯éš”300msé‡æ–°è§¦å‘ä¸€æ¬¡

/**
 * TensorFlow.js å…ƒéŸ³æ£€æµ‹å™¨ Composable
 * ä½¿ç”¨ CNN æ¨¡å‹è¿›è¡Œå…ƒéŸ³è¯†åˆ«
 * 
 * @example
 * ```ts
 * const { confirmedVowel, start, stop, onVowelDetected } = useVowelDetectorML({
 *   modelPath: '/models/vowel/model.json'
 * });
 * 
 * onVowelDetected((vowel, result) => {
 *   console.log(`æ£€æµ‹åˆ°: ${vowel}, ç½®ä¿¡åº¦: ${result.confidence}`);
 * });
 * 
 * await start();
 * ```
 */
export function useVowelDetectorML(config?: VowelDetectorConfig): VowelDetectorHookReturn {
  // ==================== é…ç½®åˆå¹¶ ====================
  const cfg: Required<Omit<VowelDetectorConfig, 'modelPath'>> & { modelPath?: string } = {
    ...DEFAULT_VOWEL_DETECTOR_CONFIG,
    ...config,
    vowelFormants: {
      ...DEFAULT_VOWEL_FORMANTS,
      ...config?.vowelFormants
    }
  };

  // ==================== å“åº”å¼çŠ¶æ€ ====================
  const currentResult = ref<VowelDetectionResult | null>(null);
  const confirmedVowel = ref<Vowel | null>(null);
  const isListening = ref(false);
  const isInitialized = ref(false);
  const error = ref<string | null>(null);
  const latestProbabilities = ref<number[] | null>(null);  // æœ€æ–°çš„å„ç±»åˆ«æ¦‚ç‡
  const debugData = shallowRef<VowelDetectorDebugData>({
    frequencyData: null,
    timeData: null
  });

  // ==================== å†…éƒ¨çŠ¶æ€ ====================
  let audioContext: AudioContext | null = null;
  let mediaStream: MediaStream | null = null;
  let workletNode: AudioWorkletNode | null = null;
  let model: tf.GraphModel | null = null;
  let pendingAudioBuffer: Float32Array | null = null; // æœ€æ–°ä¸€å¸§æ¥è‡ª AudioWorklet
  let animationFrameId: number | null = null;
  let actualSampleRate = 44100;
  
  // å…ƒéŸ³æ£€æµ‹çŠ¶æ€
  let lastConfirmedVowel: Vowel | null = null;
  let hadGapSinceLastEmit = true;
  let stableVowel: Vowel | null = null;
  let stableProb = 0;
  let lastEmitTime = 0; // ä¸Šæ¬¡è§¦å‘ onVowelDetected çš„æ—¶é—´
  
  // é™éŸ³æ£€æµ‹çŠ¶æ€
  let silenceStartTime: number | null = null;

  // ==================== äº‹ä»¶å›è°ƒ ====================
  const vowelDetectedCallbacks: VowelDetectedCallback[] = [];
  const silenceCallbacks: SilenceCallback[] = [];
  const errorCallbacks: ErrorCallback[] = [];

  function onVowelDetected(callback: VowelDetectedCallback): void {
    vowelDetectedCallbacks.push(callback);
  }

  function onSilence(callback: SilenceCallback): void {
    silenceCallbacks.push(callback);
  }

  function onError(callback: ErrorCallback): void {
    errorCallbacks.push(callback);
  }

  function emitVowelDetected(vowel: Vowel, result: VowelDetectionResult): void {
    vowelDetectedCallbacks.forEach(cb => cb(vowel, result));
  }

  function emitSilence(duration: number): void {
    silenceCallbacks.forEach(cb => cb(duration));
  }

  function emitError(err: Error): void {
    errorCallbacks.forEach(cb => cb(err));
  }

  // ==================== æ¨¡å‹åˆå§‹åŒ– ====================
  async function loadModel(): Promise<void> {
    try {
      const modelPath = config?.modelPath ?? '/models/vowel/model.json';
      model = (await tf.loadGraphModel(modelPath)) as tf.GraphModel;
      console.log('âœ… å…ƒéŸ³è¯†åˆ«æ¨¡å‹å·²åŠ è½½');
    } catch (err) {
      const e = err instanceof Error ? err : new Error(String(err));
      error.value = `æ¨¡å‹åŠ è½½å¤±è´¥: ${e.message}`;
      emitError(e);
      throw e;
    }
  }

  // ==================== éŸ³é¢‘åˆå§‹åŒ– ====================
  async function initAudio(): Promise<void> {
    try {
      // è¯·æ±‚éº¦å…‹é£æƒé™
      mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡ï¼ˆiOS Safari å…¼å®¹ï¼šwebkit å‰ç¼€ï¼‰
      const AudioCtx = window.AudioContext ?? (window as unknown as Record<string, unknown>).webkitAudioContext as typeof AudioContext;
      audioContext = new AudioCtx();

      // iOS Safari: AudioContext åˆå§‹åˆ›å»ºæ—¶å¯èƒ½æ˜¯ suspended çŠ¶æ€
      if (audioContext.state === 'suspended') {
        await audioContext.resume();
      }

      actualSampleRate = audioContext.sampleRate;
      console.log(`ğŸ“Š å®é™…é‡‡æ ·ç‡: ${actualSampleRate} Hz`);

      // æ³¨å†Œ AudioWorkletï¼ˆè¿è¡Œåœ¨ç‹¬ç«‹éŸ³é¢‘çº¿ç¨‹ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹ï¼‰
      await audioContext.audioWorklet.addModule(workletUrl);

      const source = audioContext.createMediaStreamSource(mediaStream);
      workletNode = new AudioWorkletNode(audioContext, 'vowel-audio-processor');

      // æ¥æ”¶ AudioWorklet çº¿ç¨‹å‘æ¥çš„é‡é‡‡æ ·éŸ³é¢‘æ•°æ®
      workletNode.port.onmessage = (e: MessageEvent) => {
        if (e.data?.type === 'audio') {
          pendingAudioBuffer = e.data.buffer as Float32Array;
        }
      };

      source.connect(workletNode);
      // AudioWorklet ä¸éœ€è¦è¿æ¥åˆ° destinationï¼ˆä¸æ’­æ”¾ï¼‰
      // ä½†æŸäº›æµè§ˆå™¨éœ€è¦è¿ä¸€ä¸ª sink æ‰èƒ½æŒç»­è¿è¡Œ
      workletNode.connect(audioContext.destination);

      isInitialized.value = true;
      error.value = null;
    } catch (err) {
      const e = err instanceof Error ? err : new Error(String(err));
      error.value = `éº¦å…‹é£åˆå§‹åŒ–å¤±è´¥: ${e.message}`;
      emitError(e);
      throw e;
    }
  }

  // ==================== éŸ³é¢‘é¢„å¤„ç†å’Œæ¨ç† ====================
  async function analyzeAudio(): Promise<void> {
    if (!model || !isListening.value) return;

    // å¦‚æœæ²¡æœ‰æ–°çš„éŸ³é¢‘å¸§ï¼Œè·³è¿‡æœ¬è½®
    const audioData = pendingAudioBuffer;
    if (!audioData) {
      animationFrameId = requestAnimationFrame(analyzeAudio);
      return;
    }
    pendingAudioBuffer = null; // æ¶ˆè´¹æ‰

    const now = performance.now();

    try {

      // è®¡ç®—éŸ³é‡
      const volume = calculateVolume(audioData);
      debugData.value = { frequencyData: null, timeData: audioData };

      // åˆ¤æ–­æ˜¯å¦é™éŸ³
      if (volume < cfg.silenceThreshold) {
        handleSilence(now, volume);
      } else {
        // é‡ç½®é™éŸ³è®¡æ—¶
        silenceStartTime = null;

        // è½¬æ¢ä¸º Tensor å¹¶è¿›è¡Œæ¨ç†
        const input = tf.tensor2d(audioData, [1, INPUT_SAMPLES]);
        const predictions = model!.predict(input) as tf.Tensor;
        const probabilities = await predictions.data();
        
        // æ›´æ–°æœ€æ–°æ¦‚ç‡åˆ†å¸ƒï¼ˆç”¨äº UI æ˜¾ç¤ºï¼‰
        latestProbabilities.value = Array.from(probabilities);
        
        // è·å–æœ€é«˜ç½®ä¿¡åº¦çš„ç±»
        let maxIdx = 0;
        let maxProb = 0;
        for (let i = 0; i < probabilities.length; i++) {
          if (probabilities[i] > maxProb) {
            maxProb = probabilities[i];
            maxIdx = i;
          }
        }

        const candidate = VOWEL_CLASSES[maxIdx] as Vowel;
        const candidateProb = Math.min(1, Math.max(0, maxProb));

        // Schmitt è§¦å‘å¼æ»å›ï¼šå‡å°‘å…ƒéŸ³æŠ–åŠ¨
        if (stableVowel === null) {
          if (candidateProb >= HYSTERESIS_HIGH) {
            stableVowel = candidate;
            stableProb = candidateProb;
          }
        } else if (candidate === stableVowel) {
          stableProb = candidateProb;
          if (candidateProb < HYSTERESIS_LOW) {
            stableVowel = null;
            stableProb = 0;
          }
        } else {
          // å…³é”®ä¿®å¤ï¼šç”¨å½“å‰å¸§ä¸­æ—§å…ƒéŸ³çš„å®é™…æ¦‚ç‡æ›´æ–° stableProb
          // å¦åˆ™ stableProb ä¼šåœç•™åœ¨å†å²å³°å€¼ï¼Œå¯¼è‡´æ°¸è¿œæ— æ³•åˆ‡æ¢
          const stableVowelIdx = VOWEL_CLASSES.indexOf(stableVowel!);
          if (stableVowelIdx >= 0 && stableVowelIdx < probabilities.length) {
            stableProb = probabilities[stableVowelIdx];
          }
          // æ—§å…ƒéŸ³æ¦‚ç‡è¡°å‡åˆ°é˜ˆå€¼ä»¥ä¸‹æ—¶ï¼Œç›´æ¥æ¸…é™¤
          if (stableProb < HYSTERESIS_LOW) {
            stableVowel = null;
            stableProb = 0;
            // å¦‚æœæ–°å€™é€‰è¶…è¿‡é˜ˆå€¼ï¼Œç«‹å³é‡‡çº³
            if (candidateProb >= HYSTERESIS_HIGH) {
              stableVowel = candidate;
              stableProb = candidateProb;
            }
          } else {
            // æ—§å…ƒéŸ³ä»ç„¶å­˜åœ¨ï¼Œä½†æ–°å€™é€‰è¶…è¿‡æ—§å…ƒéŸ³+ä½™é‡æ—¶åˆ‡æ¢
            const canSwitch = candidateProb >= HYSTERESIS_HIGH &&
              candidateProb >= stableProb + SWITCH_MARGIN;
            if (canSwitch) {
              stableVowel = candidate;
              stableProb = candidateProb;
            }
          }
        }

        const vowel = stableVowel;
        const confidence = stableVowel ? stableProb : candidateProb;

        // ç¡®å®šæ£€æµ‹çŠ¶æ€
        const status: DetectionStatus =
          vowel !== null && confidence > 0.5 ? 'detected' :
          vowel !== null ? 'ambiguous' : 'noise';

        const result: VowelDetectionResult = {
          vowel: status === 'detected' ? vowel : null,
          status,
          confidence,
          formants: { f1: 0, f2: 0 }, // ML æ¨¡å‹ä¸ç›´æ¥è¾“å‡ºå…±æŒ¯å³°
          volume,
          timestamp: now
        };
        currentResult.value = result;

        // å¤„ç†å…ƒéŸ³æ£€æµ‹ç»“æœ
        if (status === 'detected' && vowel !== null) {
          handleVowelDetected(vowel, result);
        }

        // æ¸…ç†
        input.dispose();
        predictions.dispose();
      }
    } catch (err) {
      const e = err instanceof Error ? err : new Error(String(err));
      console.error('æ¨ç†é”™è¯¯:', e);
      emitError(e);
    }

    animationFrameId = requestAnimationFrame(analyzeAudio);
  }

  // ==================== éŸ³é‡è®¡ç®— ====================
  function calculateVolume(audioData: Float32Array): number {
    let sum = 0;
    for (let i = 0; i < audioData.length; i++) {
      sum += audioData[i] * audioData[i];
    }
    const rms = Math.sqrt(sum / audioData.length);
    const db = rms > 0 ? 20 * Math.log10(rms) : -100;
    return Math.max(-100, Math.min(0, db)); // é™åˆ¶èŒƒå›´åœ¨ -100 åˆ° 0
  }

  // ==================== å…ƒéŸ³æ£€æµ‹å¤„ç† ====================
  function handleVowelDetected(vowel: Vowel, result: VowelDetectionResult): void {
    const isNewVowel = vowel !== lastConfirmedVowel;
    const now = performance.now();
    // æŒç»­å‘åŒä¸€å…ƒéŸ³æ—¶ï¼Œæ¯éš”ä¸€æ®µæ—¶é—´é‡æ–°è§¦å‘ï¼ˆæ”¯æŒåºåˆ—ä¸­è¿ç»­ç›¸åŒå…ƒéŸ³å¦‚ I,I,Iï¼‰
    const sustainedReEmit = !isNewVowel && !hadGapSinceLastEmit 
      && (now - lastEmitTime >= SUSTAINED_RE_EMIT_INTERVAL);
    
    // è§¦å‘æ¡ä»¶ï¼šå…ƒéŸ³å˜åŒ– / ç»è¿‡äº†é™éŸ³é—´éš” / æŒç»­å‘éŸ³é‡æ–°è§¦å‘
    if (isNewVowel || hadGapSinceLastEmit || sustainedReEmit) {
      confirmedVowel.value = vowel;
      lastConfirmedVowel = vowel;
      hadGapSinceLastEmit = false;
      lastEmitTime = now;
      emitVowelDetected(vowel, result);
    }
  }

  // ==================== é™éŸ³å¤„ç† ====================
  function handleSilence(now: number, volume: number): void {
    if (silenceStartTime === null) {
      silenceStartTime = now;
    } else {
      emitSilence(now - silenceStartTime);
    }
    
    currentResult.value = {
      vowel: null,
      status: 'silence',
      confidence: 0,
      formants: { f1: 0, f2: 0 },
      volume,
      timestamp: now
    };
    
    hadGapSinceLastEmit = true;
    confirmedVowel.value = null;
    lastConfirmedVowel = null;
    lastEmitTime = 0;
    // é‡ç½® Schmitt è§¦å‘å™¨çŠ¶æ€ï¼Œé˜²æ­¢æ—§å…ƒéŸ³å¹²æ‰°ä¸‹æ¬¡æ£€æµ‹
    stableVowel = null;
    stableProb = 0;
  }

  // ==================== æ§åˆ¶æ–¹æ³• ====================
  async function start(): Promise<void> {
    if (isListening.value) return;

    if (!isInitialized.value) {
      await initAudio();
      await loadModel();
    }

    // ç¡®ä¿ AudioContext å¤„äºè¿è¡ŒçŠ¶æ€
    if (audioContext?.state === 'suspended') {
      await audioContext.resume();
    }

    isListening.value = true;
    silenceStartTime = null;
    pendingAudioBuffer = null;
    
    // å¼€å§‹åˆ†æå¾ªç¯
    analyzeAudio();
  }

  function stop(): void {
    isListening.value = false;
    
    if (animationFrameId !== null) {
      cancelAnimationFrame(animationFrameId);
      animationFrameId = null;
    }
    
    confirmedVowel.value = null;
    lastConfirmedVowel = null;
    hadGapSinceLastEmit = true;
    lastEmitTime = 0;
    stableVowel = null;
    stableProb = 0;
  }

  function reset(): void {
    stop();
    
    // é‡Šæ”¾èµ„æº
    if (mediaStream) {
      mediaStream.getTracks().forEach(track => track.stop());
      mediaStream = null;
    }
    
    if (audioContext) {
      audioContext.close();
      audioContext = null;
    }
    
    if (model) {
      model.dispose();
      model = null;
    }

    if (workletNode) {
      workletNode.disconnect();
      workletNode.port.onmessage = null;
      workletNode = null;
    }

    pendingAudioBuffer = null;
    
    isInitialized.value = false;
    currentResult.value = null;
    error.value = null;
    latestProbabilities.value = null;
  }

  // ==================== è¯Šæ–­å’Œè°ƒè¯• ====================
  /**
   * è·å–éŸ³é¢‘æ ¼å¼è¯Šæ–­ä¿¡æ¯
   * ç”¨äºè°ƒè¯•é‡‡æ ·ç‡ä¸åŒ¹é…é—®é¢˜
   */
  function getAudioDiagnostics() {
    return {
      detectorType: 'ml',
      targetSampleRate: TARGET_SAMPLE_RATE,
      actualSampleRate: actualSampleRate,
      inputSamples: INPUT_SAMPLES,
      expectedDurationMs: (INPUT_SAMPLES / TARGET_SAMPLE_RATE) * 1000,
      audioContextState: audioContext?.state,
      audioProcessing: 'AudioWorklet',
      silenceThreshold: cfg.silenceThreshold,
      modelPath: config?.modelPath ?? '/models/vowel/model.json',
      isInitialized: isInitialized.value,
      isListening: isListening.value
    };
  }

  // ==================== ç”Ÿå‘½å‘¨æœŸ ====================
  onUnmounted(() => {
    reset();
  });

  // ==================== è¿”å› ====================
  return {
    currentResult,
    confirmedVowel,
    isListening,
    isInitialized,
    error,
    latestProbabilities,
    start,
    stop,
    reset,
    onVowelDetected,
    onSilence,
    onError,
    debugData,
    getAudioDiagnostics
  };
}
