import { ref, shallowRef, onUnmounted } from 'vue';
import * as tf from '@tensorflow/tfjs';
import type {
  Vowel,
  VowelDetectorConfig,
  VowelDetectionResult,
  DetectionStatus,
  VowelDetectedCallback,
  SilenceCallback,
  ErrorCallback,
  VowelDetectorHookReturn,
  VowelDetectorDebugData
} from '@/types/game';
import { DEFAULT_VOWEL_DETECTOR_CONFIG, DEFAULT_VOWEL_FORMANTS } from '@/config/vowels';

const VOWEL_CLASSES = ['A', 'E', 'I', 'O', 'U', 'silence'] as const;
const INPUT_SAMPLES = 3360; // 210ms @ 16kHz
const TARGET_SAMPLE_RATE = 16000; // è®­ç»ƒæ¨¡å‹ä½¿ç”¨çš„é‡‡æ ·ç‡
const HYSTERESIS_HIGH = 0.6;
const HYSTERESIS_LOW = 0.45;
const SWITCH_MARGIN = 0.08;
const SUSTAINED_RE_EMIT_INTERVAL = 300; // æŒç»­å‘åŒä¸€å…ƒéŸ³æ—¶ï¼Œæ¯éš”300msé‡æ–°è§¦å‘ä¸€æ¬¡

/**
 * TensorFlow.js å…ƒéŸ³æ£€æµ‹å™¨ Composable
 * ä½¿ç”¨ CNN æ¨¡å‹è¿›è¡Œå…ƒéŸ³è¯†åˆ«
 * 
 * @example
 * ```ts
 * const { confirmedVowel, start, stop, onVowelDetected } = useVowelDetectorML({
 *   modelPath: '/models/vowel/model.json'
 * });
 * 
 * onVowelDetected((vowel, result) => {
 *   console.log(`æ£€æµ‹åˆ°: ${vowel}, ç½®ä¿¡åº¦: ${result.confidence}`);
 * });
 * 
 * await start();
 * ```
 */
export function useVowelDetectorML(config?: VowelDetectorConfig): VowelDetectorHookReturn {
  // ==================== é…ç½®åˆå¹¶ ====================
  const cfg: Required<Omit<VowelDetectorConfig, 'modelPath'>> & { modelPath?: string } = {
    ...DEFAULT_VOWEL_DETECTOR_CONFIG,
    ...config,
    vowelFormants: {
      ...DEFAULT_VOWEL_FORMANTS,
      ...config?.vowelFormants
    }
  };

  // ==================== å“åº”å¼çŠ¶æ€ ====================
  const currentResult = ref<VowelDetectionResult | null>(null);
  const confirmedVowel = ref<Vowel | null>(null);
  const isListening = ref(false);
  const isInitialized = ref(false);
  const error = ref<string | null>(null);
  const latestProbabilities = ref<number[] | null>(null);  // æœ€æ–°çš„å„ç±»åˆ«æ¦‚ç‡
  const debugData = shallowRef<VowelDetectorDebugData>({
    frequencyData: null,
    timeData: null
  });

  // ==================== å†…éƒ¨çŠ¶æ€ ====================
  let audioContext: AudioContext | null = null;
  let mediaStream: MediaStream | null = null;
  let model: tf.GraphModel | null = null;
  let audioBuffer: Float32Array | null = null;
  let bufferIndex = 0;
  let animationFrameId: number | null = null;
  let actualSampleRate = 44100; // å®é™…é‡‡æ ·ç‡ï¼ˆä¼šåœ¨åˆå§‹åŒ–æ—¶æ£€æµ‹ï¼‰
  let resampleRatio = 1; // é‡é‡‡æ ·æ¯”ä¾‹
  
  // å…ƒéŸ³æ£€æµ‹çŠ¶æ€
  let lastConfirmedVowel: Vowel | null = null;
  let hadGapSinceLastEmit = true;
  let stableVowel: Vowel | null = null;
  let stableProb = 0;
  let lastEmitTime = 0; // ä¸Šæ¬¡è§¦å‘ onVowelDetected çš„æ—¶é—´
  
  // é™éŸ³æ£€æµ‹çŠ¶æ€
  let silenceStartTime: number | null = null;

  // ==================== äº‹ä»¶å›è°ƒ ====================
  const vowelDetectedCallbacks: VowelDetectedCallback[] = [];
  const silenceCallbacks: SilenceCallback[] = [];
  const errorCallbacks: ErrorCallback[] = [];

  function onVowelDetected(callback: VowelDetectedCallback): void {
    vowelDetectedCallbacks.push(callback);
  }

  function onSilence(callback: SilenceCallback): void {
    silenceCallbacks.push(callback);
  }

  function onError(callback: ErrorCallback): void {
    errorCallbacks.push(callback);
  }

  function emitVowelDetected(vowel: Vowel, result: VowelDetectionResult): void {
    vowelDetectedCallbacks.forEach(cb => cb(vowel, result));
  }

  function emitSilence(duration: number): void {
    silenceCallbacks.forEach(cb => cb(duration));
  }

  function emitError(err: Error): void {
    errorCallbacks.forEach(cb => cb(err));
  }

  // ==================== æ¨¡å‹åˆå§‹åŒ– ====================
  async function loadModel(): Promise<void> {
    try {
      const modelPath = config?.modelPath ?? '/models/vowel/model.json';
      model = (await tf.loadGraphModel(modelPath)) as tf.GraphModel;
      console.log('âœ… å…ƒéŸ³è¯†åˆ«æ¨¡å‹å·²åŠ è½½');
    } catch (err) {
      const e = err instanceof Error ? err : new Error(String(err));
      error.value = `æ¨¡å‹åŠ è½½å¤±è´¥: ${e.message}`;
      emitError(e);
      throw e;
    }
  }

  // ==================== éŸ³é¢‘åˆå§‹åŒ– ====================
  async function initAudio(): Promise<void> {
    try {
      // è¯·æ±‚éº¦å…‹é£æƒé™
      mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡ï¼ˆä¸æŒ‡å®šé‡‡æ ·ç‡ï¼Œè®©ç³»ç»Ÿä½¿ç”¨é»˜è®¤å€¼ï¼‰
      audioContext = new AudioContext();
      
      // âš ï¸ å…³é”®ï¼šè·å–å®é™…é‡‡æ ·ç‡
      actualSampleRate = audioContext.sampleRate;
      resampleRatio = TARGET_SAMPLE_RATE / actualSampleRate;
      
      console.log(`ğŸ“Š å®é™…é‡‡æ ·ç‡: ${actualSampleRate} Hz`);
      console.log(`ğŸ“Š ç›®æ ‡é‡‡æ ·ç‡: ${TARGET_SAMPLE_RATE} Hz`);
      console.log(`ğŸ“Š é‡é‡‡æ ·æ¯”ä¾‹: ${resampleRatio.toFixed(4)}`);
      
      // è®¡ç®—é‡é‡‡æ ·åçš„ç¼“å†²åŒºå¤§å°
      // å¦‚æœå®é™…é‡‡æ ·ç‡æ˜¯ 44100Hzï¼Œé‡é‡‡æ ·åˆ° 16000Hz åï¼Œ
      // æ¯ä¸ª 4096 æ ·æœ¬çš„éŸ³é¢‘å—ä¼šå˜æˆ ~1495 æ ·æœ¬
      // åˆ›å»ºéŸ³é¢‘ç¼“å†²åŒºï¼ˆç”¨äºå­˜å‚¨é‡é‡‡æ ·åçš„æ•°æ®ï¼‰
      audioBuffer = new Float32Array(INPUT_SAMPLES);
      bufferIndex = 0;

      // åˆ›å»º ScriptProcessorNode ç”¨äºæ”¶é›†éŸ³é¢‘æ•°æ®
      // ä½¿ç”¨ 2048 æ ·æœ¬çš„ç¼“å†²å¤§å°ï¼Œé™ä½å»¶è¿Ÿï¼ˆ~46ms @ 44100Hzï¼‰
      const scriptNode = audioContext.createScriptProcessor(2048, 1, 1);
      
      scriptNode.onaudioprocess = (event: AudioProcessingEvent) => {
        const inputData = event.inputBuffer.getChannelData(0);
        
        // âš ï¸ æ­£ç¡®çš„é‡é‡‡æ ·ï¼šä»é«˜é‡‡æ ·ç‡é™åˆ°ä½é‡‡æ ·ç‡
        // ä¾‹å¦‚ï¼š44100Hz -> 16000Hzï¼Œæ¯ 2.76 ä¸ªæºæ ·æœ¬äº§ç”Ÿ 1 ä¸ªç›®æ ‡æ ·æœ¬
        const resampledLength = Math.ceil(inputData.length * resampleRatio);
        
        for (let i = 0; i < resampledLength; i++) {
          // è®¡ç®—åœ¨æºæ•°ç»„ä¸­çš„ä½ç½®
          const sourcePos = i / resampleRatio;
          const intPart = Math.floor(sourcePos);
          const fracPart = sourcePos - intPart;
          
          // çº¿æ€§æ’å€¼
          let sample: number;
          if (intPart >= inputData.length - 1) {
            sample = inputData[inputData.length - 1];
          } else {
            sample = inputData[intPart] * (1 - fracPart) + inputData[intPart + 1] * fracPart;
          }
          
          // å°†é‡é‡‡æ ·åçš„æ•°æ®æ”¾å…¥ä¸»ç¼“å†²åŒºï¼ˆå¾ªç¯ç¼“å†²ï¼‰
          audioBuffer![bufferIndex] = sample;
          bufferIndex = (bufferIndex + 1) % INPUT_SAMPLES;
        }
      };

      const source = audioContext.createMediaStreamSource(mediaStream);
      source.connect(scriptNode);
      scriptNode.connect(audioContext.destination);

      isInitialized.value = true;
      error.value = null;
    } catch (err) {
      const e = err instanceof Error ? err : new Error(String(err));
      error.value = `éº¦å…‹é£åˆå§‹åŒ–å¤±è´¥: ${e.message}`;
      emitError(e);
      throw e;
    }
  }

  // ==================== éŸ³é¢‘é¢„å¤„ç†å’Œæ¨ç† ====================
  async function analyzeAudio(): Promise<void> {
    if (!model || !audioBuffer || !isListening.value) return;

    const now = performance.now();

    try {
      // ä»å¾ªç¯ç¼“å†²åŒºæ­£ç¡®è¯»å–æ•°æ®
      // audioBuffer æ˜¯å¾ªç¯ç¼“å†²åŒºï¼ŒbufferIndex æŒ‡å‘ä¸‹ä¸€ä¸ªè¦å†™å…¥çš„ä½ç½®
      // æ­£ç¡®çš„é¡ºåºæ˜¯ï¼š[bufferIndex...end] + [0...bufferIndex-1]
      const audioData = new Float32Array(INPUT_SAMPLES);
      for (let i = 0; i < INPUT_SAMPLES; i++) {
        // ä» bufferIndex å¼€å§‹è¯»å–ï¼Œå›ç»•åˆ°å¼€å¤´
        audioData[i] = audioBuffer[(bufferIndex + i) % INPUT_SAMPLES];
      }

      // è®¡ç®—éŸ³é‡
      const volume = calculateVolume(audioData);
      debugData.value = { frequencyData: null, timeData: audioData };

      // åˆ¤æ–­æ˜¯å¦é™éŸ³
      if (volume < cfg.silenceThreshold) {
        handleSilence(now, volume);
      } else {
        // é‡ç½®é™éŸ³è®¡æ—¶
        silenceStartTime = null;

        // è½¬æ¢ä¸º Tensor å¹¶è¿›è¡Œæ¨ç†
        const input = tf.tensor2d(audioData, [1, INPUT_SAMPLES]);
        const predictions = model!.predict(input) as tf.Tensor;
        const probabilities = await predictions.data();
        
        // æ›´æ–°æœ€æ–°æ¦‚ç‡åˆ†å¸ƒï¼ˆç”¨äº UI æ˜¾ç¤ºï¼‰
        latestProbabilities.value = Array.from(probabilities);
        
        // è·å–æœ€é«˜ç½®ä¿¡åº¦çš„ç±»
        let maxIdx = 0;
        let maxProb = 0;
        for (let i = 0; i < probabilities.length; i++) {
          if (probabilities[i] > maxProb) {
            maxProb = probabilities[i];
            maxIdx = i;
          }
        }

        const candidate = VOWEL_CLASSES[maxIdx] as Vowel;
        const candidateProb = Math.min(1, Math.max(0, maxProb));

        // Schmitt è§¦å‘å¼æ»å›ï¼šå‡å°‘å…ƒéŸ³æŠ–åŠ¨
        if (stableVowel === null) {
          if (candidateProb >= HYSTERESIS_HIGH) {
            stableVowel = candidate;
            stableProb = candidateProb;
          }
        } else if (candidate === stableVowel) {
          stableProb = candidateProb;
          if (candidateProb < HYSTERESIS_LOW) {
            stableVowel = null;
            stableProb = 0;
          }
        } else {
          // å…³é”®ä¿®å¤ï¼šç”¨å½“å‰å¸§ä¸­æ—§å…ƒéŸ³çš„å®é™…æ¦‚ç‡æ›´æ–° stableProb
          // å¦åˆ™ stableProb ä¼šåœç•™åœ¨å†å²å³°å€¼ï¼Œå¯¼è‡´æ°¸è¿œæ— æ³•åˆ‡æ¢
          const stableVowelIdx = VOWEL_CLASSES.indexOf(stableVowel!);
          if (stableVowelIdx >= 0 && stableVowelIdx < probabilities.length) {
            stableProb = probabilities[stableVowelIdx];
          }
          // æ—§å…ƒéŸ³æ¦‚ç‡è¡°å‡åˆ°é˜ˆå€¼ä»¥ä¸‹æ—¶ï¼Œç›´æ¥æ¸…é™¤
          if (stableProb < HYSTERESIS_LOW) {
            stableVowel = null;
            stableProb = 0;
            // å¦‚æœæ–°å€™é€‰è¶…è¿‡é˜ˆå€¼ï¼Œç«‹å³é‡‡çº³
            if (candidateProb >= HYSTERESIS_HIGH) {
              stableVowel = candidate;
              stableProb = candidateProb;
            }
          } else {
            // æ—§å…ƒéŸ³ä»ç„¶å­˜åœ¨ï¼Œä½†æ–°å€™é€‰è¶…è¿‡æ—§å…ƒéŸ³+ä½™é‡æ—¶åˆ‡æ¢
            const canSwitch = candidateProb >= HYSTERESIS_HIGH &&
              candidateProb >= stableProb + SWITCH_MARGIN;
            if (canSwitch) {
              stableVowel = candidate;
              stableProb = candidateProb;
            }
          }
        }

        const vowel = stableVowel;
        const confidence = stableVowel ? stableProb : candidateProb;

        // ç¡®å®šæ£€æµ‹çŠ¶æ€
        const status: DetectionStatus =
          vowel !== null && confidence > 0.5 ? 'detected' :
          vowel !== null ? 'ambiguous' : 'noise';

        const result: VowelDetectionResult = {
          vowel: status === 'detected' ? vowel : null,
          status,
          confidence,
          formants: { f1: 0, f2: 0 }, // ML æ¨¡å‹ä¸ç›´æ¥è¾“å‡ºå…±æŒ¯å³°
          volume,
          timestamp: now
        };
        currentResult.value = result;

        // å¤„ç†å…ƒéŸ³æ£€æµ‹ç»“æœ
        if (status === 'detected' && vowel !== null) {
          handleVowelDetected(vowel, result);
        }

        // æ¸…ç†
        input.dispose();
        predictions.dispose();
      }
    } catch (err) {
      const e = err instanceof Error ? err : new Error(String(err));
      console.error('æ¨ç†é”™è¯¯:', e);
      emitError(e);
    }

    animationFrameId = requestAnimationFrame(analyzeAudio);
  }

  // ==================== éŸ³é‡è®¡ç®— ====================
  function calculateVolume(audioData: Float32Array): number {
    let sum = 0;
    for (let i = 0; i < audioData.length; i++) {
      sum += audioData[i] * audioData[i];
    }
    const rms = Math.sqrt(sum / audioData.length);
    const db = rms > 0 ? 20 * Math.log10(rms) : -100;
    return Math.max(-100, Math.min(0, db)); // é™åˆ¶èŒƒå›´åœ¨ -100 åˆ° 0
  }

  // ==================== å…ƒéŸ³æ£€æµ‹å¤„ç† ====================
  function handleVowelDetected(vowel: Vowel, result: VowelDetectionResult): void {
    const isNewVowel = vowel !== lastConfirmedVowel;
    const now = performance.now();
    // æŒç»­å‘åŒä¸€å…ƒéŸ³æ—¶ï¼Œæ¯éš”ä¸€æ®µæ—¶é—´é‡æ–°è§¦å‘ï¼ˆæ”¯æŒåºåˆ—ä¸­è¿ç»­ç›¸åŒå…ƒéŸ³å¦‚ I,I,Iï¼‰
    const sustainedReEmit = !isNewVowel && !hadGapSinceLastEmit 
      && (now - lastEmitTime >= SUSTAINED_RE_EMIT_INTERVAL);
    
    // è§¦å‘æ¡ä»¶ï¼šå…ƒéŸ³å˜åŒ– / ç»è¿‡äº†é™éŸ³é—´éš” / æŒç»­å‘éŸ³é‡æ–°è§¦å‘
    if (isNewVowel || hadGapSinceLastEmit || sustainedReEmit) {
      confirmedVowel.value = vowel;
      lastConfirmedVowel = vowel;
      hadGapSinceLastEmit = false;
      lastEmitTime = now;
      emitVowelDetected(vowel, result);
    }
  }

  // ==================== é™éŸ³å¤„ç† ====================
  function handleSilence(now: number, volume: number): void {
    if (silenceStartTime === null) {
      silenceStartTime = now;
    } else {
      emitSilence(now - silenceStartTime);
    }
    
    currentResult.value = {
      vowel: null,
      status: 'silence',
      confidence: 0,
      formants: { f1: 0, f2: 0 },
      volume,
      timestamp: now
    };
    
    hadGapSinceLastEmit = true;
    confirmedVowel.value = null;
    lastConfirmedVowel = null;
    lastEmitTime = 0;
    // é‡ç½® Schmitt è§¦å‘å™¨çŠ¶æ€ï¼Œé˜²æ­¢æ—§å…ƒéŸ³å¹²æ‰°ä¸‹æ¬¡æ£€æµ‹
    stableVowel = null;
    stableProb = 0;
  }

  // ==================== æ§åˆ¶æ–¹æ³• ====================
  async function start(): Promise<void> {
    if (isListening.value) return;

    if (!isInitialized.value) {
      await initAudio();
      await loadModel();
    }

    // ç¡®ä¿ AudioContext å¤„äºè¿è¡ŒçŠ¶æ€
    if (audioContext?.state === 'suspended') {
      await audioContext.resume();
    }

    isListening.value = true;
    silenceStartTime = null;
    bufferIndex = 0;
    
    // å¼€å§‹åˆ†æå¾ªç¯
    analyzeAudio();
  }

  function stop(): void {
    isListening.value = false;
    
    if (animationFrameId !== null) {
      cancelAnimationFrame(animationFrameId);
      animationFrameId = null;
    }
    
    confirmedVowel.value = null;
    lastConfirmedVowel = null;
    hadGapSinceLastEmit = true;
    lastEmitTime = 0;
    stableVowel = null;
    stableProb = 0;
  }

  function reset(): void {
    stop();
    
    // é‡Šæ”¾èµ„æº
    if (mediaStream) {
      mediaStream.getTracks().forEach(track => track.stop());
      mediaStream = null;
    }
    
    if (audioContext) {
      audioContext.close();
      audioContext = null;
    }
    
    if (model) {
      model.dispose();
      model = null;
    }

    audioBuffer = null;
    bufferIndex = 0;
    
    isInitialized.value = false;
    currentResult.value = null;
    error.value = null;
    latestProbabilities.value = null;
  }

  // ==================== è¯Šæ–­å’Œè°ƒè¯• ====================
  /**
   * è·å–éŸ³é¢‘æ ¼å¼è¯Šæ–­ä¿¡æ¯
   * ç”¨äºè°ƒè¯•é‡‡æ ·ç‡ä¸åŒ¹é…é—®é¢˜
   */
  function getAudioDiagnostics() {
    return {
      detectorType: 'ml',
      targetSampleRate: TARGET_SAMPLE_RATE,
      actualSampleRate: actualSampleRate,
      resampleRatio: resampleRatio,
      inputSamples: INPUT_SAMPLES,
      expectedDurationMs: (INPUT_SAMPLES / TARGET_SAMPLE_RATE) * 1000,
      actualDurationMs: (INPUT_SAMPLES / actualSampleRate) * 1000,
      audioContextState: audioContext?.state,
      silenceThreshold: cfg.silenceThreshold,
      modelPath: config?.modelPath ?? '/models/vowel/model.json',
      isInitialized: isInitialized.value,
      isListening: isListening.value
    };
  }

  // ==================== ç”Ÿå‘½å‘¨æœŸ ====================
  onUnmounted(() => {
    reset();
  });

  // ==================== è¿”å› ====================
  return {
    currentResult,
    confirmedVowel,
    isListening,
    isInitialized,
    error,
    latestProbabilities,
    start,
    stop,
    reset,
    onVowelDetected,
    onSilence,
    onError,
    debugData,
    getAudioDiagnostics
  };
}
